{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a13ddc8",
   "metadata": {},
   "source": [
    "# Inferencia con LLamaCpp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623f0cfe",
   "metadata": {},
   "source": [
    "## Instalación de paquetes de python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885dabeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install  prompttools typing sentence-transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4571c8",
   "metadata": {},
   "source": [
    "# Instalación de llama-cpp-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06854167",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting llama-cpp-python\n",
      "  Downloading llama_cpp_python-0.2.78.tar.gz (50.2 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.2/50.2 MB\u001b[0m \u001b[31m115.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m1m115.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting typing-extensions>=4.5.0 (from llama-cpp-python)\n",
      "  Downloading typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting numpy>=1.20.0 (from llama-cpp-python)\n",
      "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m357.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting diskcache>=5.6.1 (from llama-cpp-python)\n",
      "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting jinja2>=2.11.3 (from llama-cpp-python)\n",
      "  Downloading jinja2-3.1.4-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2>=2.11.3->llama-cpp-python)\n",
      "  Downloading MarkupSafe-2.1.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
      "Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m361.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading jinja2-3.1.4-py3-none-any.whl (133 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.3/133.3 kB\u001b[0m \u001b[31m456.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m115.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m01\u001b[0m\n",
      "\u001b[?25hDownloading typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Downloading MarkupSafe-2.1.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (28 kB)\n",
      "Building wheels for collected packages: llama-cpp-python\n",
      "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.2.78-cp311-cp311-linux_x86_64.whl size=3749804 sha256=b4aa1e01f73f232c3f7742d66861428671f624218dd51af4d0bf2108686a2e7c\n",
      "  Stored in directory: /run/user/1000/pip-ephem-wheel-cache-dx0_er86/wheels/90/04/d0/e152de3e3f4ecf76f4a0c379e2806cc8a9700c0384b82cbd2f\n",
      "Successfully built llama-cpp-python\n",
      "Installing collected packages: typing-extensions, numpy, MarkupSafe, diskcache, jinja2, llama-cpp-python\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.11.0\n",
      "    Not uninstalling typing-extensions at /nix/store/k920s30a5dqzndfj4ic23p7rs0pc927k-python3.11-typing-extensions-4.11.0/lib/python3.11/site-packages, outside environment /home/guachu/Desktop/practica/inferencia_con_llamacpp/.venv\n",
      "    Can't uninstall 'typing_extensions'. No files were found to uninstall.\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.26.4\n",
      "    Uninstalling numpy-1.26.4:\n",
      "      Successfully uninstalled numpy-1.26.4\n",
      "  Attempting uninstall: MarkupSafe\n",
      "    Found existing installation: MarkupSafe 2.1.5\n",
      "    Uninstalling MarkupSafe-2.1.5:\n",
      "      Successfully uninstalled MarkupSafe-2.1.5\n",
      "  Attempting uninstall: diskcache\n",
      "    Found existing installation: diskcache 5.6.3\n",
      "    Uninstalling diskcache-5.6.3:\n",
      "      Successfully uninstalled diskcache-5.6.3\n",
      "  Attempting uninstall: jinja2\n",
      "    Found existing installation: Jinja2 3.1.4\n",
      "    Uninstalling Jinja2-3.1.4:\n",
      "      Successfully uninstalled Jinja2-3.1.4\n",
      "  Attempting uninstall: llama-cpp-python\n",
      "    Found existing installation: llama_cpp_python 0.2.78\n",
      "    Uninstalling llama_cpp_python-0.2.78:\n",
      "      Successfully uninstalled llama_cpp_python-0.2.78\n",
      "Successfully installed MarkupSafe-2.1.5 diskcache-5.6.3 jinja2-3.1.4 llama-cpp-python-0.2.78 numpy-1.26.4 typing-extensions-4.12.2\n"
     ]
    }
   ],
   "source": [
    "!CMAKE_ARGS=\"-DLLAMA_AVX=on -DLLAMA_AVX2=on -DLLAMA_F16C=on -DLLAMA_FMA=on\" .venv/bin/pip install llama-cpp-python --upgrade --force-reinstall --no-cache-dir\n",
    "\n",
    "#!CMAKE_ARGS=\"-DLLAMA_CUDA=on\" .venv/bin/pip install llama-cpp-python --force-reinstall --no-cache-dir --upgrade --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu123"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842f1e47",
   "metadata": {},
   "source": [
    "## Importamos `prompttools` y los modulos de **typings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beaa70a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Tuple\n",
    "from prompttools.experiment import LlamaCppExperiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a2fb1e",
   "metadata": {},
   "source": [
    "# Descargamos los modelos cuantizados para usar con llama-cpp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7edaa31b",
   "metadata": {},
   "source": [
    "[Referencia de tipos de cuantización empleados en llama.cpp y cómo distinguirlos](https://github.com/ggerganov/llama.cpp/wiki/Tensor-Encoding-Schemes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77f0c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p models\n",
    "!curl -sSLk https://huggingface.co/TheBloke/LLaMa-7B-GGML/blob/main/llama-7b.ggmlv3.q2_K.bin -O --output-dir models\n",
    "!curl -sSLk https://huggingface.co/TheBloke/Llama-2-7B-GGML/blob/main/llama-2-7b.ggmlv3.q2_K.bin -O --output-dir models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5baf520",
   "metadata": {},
   "source": [
    "> OJO! ¿Lo hemos descargado bien? ;-) Miremos..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb076957",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -lh models/llama*\n",
    "!file models/llama-2-7b.ggmlv3.q2_K.bin\n",
    "!file models/llama-2-7b.ggmlv3.q2_K.bin\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1844c6",
   "metadata": {},
   "source": [
    "Cuidado con las descargas directas de HF..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede1fc02",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e12278",
   "metadata": {},
   "source": [
    "En la carpeta **models**/buenos/** , hemos descargado directamente los modelos adecuados.\n",
    "\n",
    "- [TheBloke - LLama-7b GGUF](https://huggingface.co/TheBloke/LLaMA-7b-GGUF/tree/main)\n",
    "- [TheBloke - LLama-2-7b GGUF](https://huggingface.co/TheBloke/Llama-2-7B-GGUF)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e92b002",
   "metadata": {},
   "source": [
    "Si quieres saber que es el formato GGUF, que se emplea con llama.cpp, mira [este artículo](https://vickiboykis.com/2024/02/28/gguf-the-long-way-around/).\n",
    "\n",
    "Y ojo con los modelos que cargas. Mira este [vídeo explicativo](https://youtu.be/2ethDz9KnLk) sobre el tema."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8b4987",
   "metadata": {},
   "source": [
    "Ahora crearemos nuestros prompts a comparar así como la configuración de los modelos que queremos comparar y el parámetro de la temperatura."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8935a8d7",
   "metadata": {},
   "source": [
    "[LlamaCppExperiment](https://github.com/hegelai/prompttools/blob/main/prompttools/experiment/experiments/llama_cpp_experiment.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9114cfbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_paths = [\n",
    "    \"/home/guachu/Desktop/practica/inferencia_con_llamacpp/models/buenos/llama-7b.Q2_K.gguf\",  \n",
    "    \"/home/guachu/Desktop/practica/inferencia_con_llamacpp/models/buenos/llama-2-7b.Q2_K.gguf\",\n",
    "]  \n",
    "prompts = [\n",
    "    \"Who was the first president?\",\n",
    "    \"Who was the first president of the USA?\",\n",
    "]\n",
    "temperatures = [0.0, 1.0]\n",
    "max_tokens = [20]\n",
    "\n",
    "\n",
    "call_params = dict(temperature=temperatures,max_tokens=max_tokens)\n",
    "\n",
    "experiment = LlamaCppExperiment(model_paths, prompts, call_params=call_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3fa5450",
   "metadata": {},
   "source": [
    "Ejecutamos el experimento una vez que lo hemos configurado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b33130",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266c13eb",
   "metadata": {},
   "source": [
    "## Evaluación de los modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bebb8023",
   "metadata": {},
   "source": [
    "Definimos una función de evaluación. En este caso, la de similaridad semántica\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ddbb951",
   "metadata": {},
   "outputs": [],
   "source": [
    "from prompttools.utils import semantic_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80dfeec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "experiment.evaluate(\"similar_to_expected\", semantic_similarity, expected=[\"George Washington\"] * 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "974d6065",
   "metadata": {},
   "source": [
    "Finalmente, visualizamos el resultado del experimento\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d09c18e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "experiment.visualize()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
