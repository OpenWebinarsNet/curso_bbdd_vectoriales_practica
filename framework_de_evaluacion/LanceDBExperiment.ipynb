{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a13ddc8",
   "metadata": {},
   "source": [
    "# Experimento con LanceDB (prompttools)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f294c101",
   "metadata": {},
   "source": [
    "[Prompt Tools](https://github.com/hegelai/prompttools/tree/main/examples/notebooks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623f0cfe",
   "metadata": {},
   "source": [
    "## Instalación de paquetes de python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "885dabeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --quiet --force-reinstall prompttools lancedb sentence_transformers openai pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb14cef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip list | grep lance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eac35f8",
   "metadata": {},
   "source": [
    "## El propio experimento carga lancedb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7120ee",
   "metadata": {},
   "source": [
    "[LanceDBExperiment](https://github.com/hegelai/prompttools/blob/main/prompttools/experiment/experiments/lancedb_experiment.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beaa70a1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-20T07:13:56.823039Z",
     "start_time": "2023-07-20T07:13:55.927616Z"
    }
   },
   "outputs": [],
   "source": [
    "from prompttools.experiment import LanceDBExperiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622dea9a",
   "metadata": {},
   "source": [
    "## Comparando distintos embeddings con LanceDB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5650e31",
   "metadata": {},
   "source": [
    "Un caso de uso común es comparar dos modelos de embeddings diferentes y cómo puede afectar a la recuperación de documentos. Aquí podemos definir qué modelos de embeddings queremos probar.\n",
    "\n",
    "Nota: Si previamente no ha descargado estos modelos de embeddings. Esto puede iniciar las descargas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821bbb21-292c-44e5-bdf0-ab05350acb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import openai\n",
    "import os\n",
    "\n",
    "# Comprobamos si la variable de entorno de OpenAI OPENAI_API_KEY \n",
    "if \"OPENAI_API_KEY\" not in os.environ:\n",
    "    # O la podemos indicar directamente, si no tenemos la variable de entorno visible.\n",
    "    openai.api_key = \"...\"\n",
    "\n",
    "\n",
    "DEFAULT = SentenceTransformer(\"paraphrase-MiniLM-L3-v2\")\n",
    "MIMNILM_L6 = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "\n",
    "def default_embed_func(batch):\n",
    "    return [DEFAULT.encode(sentence) for sentence in batch]\n",
    "\n",
    "\n",
    "def minilm_l6_embed_func(batch):\n",
    "    return [MIMNILM_L6.encode(sentence) for sentence in batch]\n",
    "\n",
    "\n",
    "def openai_ada2_embed_func(batch):\n",
    "    rs = openai.Embedding.create(input=batch, engine=\"text-embedding-ada-002\")\n",
    "    return [record[\"embedding\"] for record in rs[\"data\"]]\n",
    "\n",
    "\n",
    "emb_fns = {\"minilm_l6\": minilm_l6_embed_func, \"default\": default_embed_func}\n",
    "# Prueba con los embeddings de openAI\n",
    "# emb_fns = {\"openai-ada-002\": openai_ada2_embed_func, \"minilm_l6\": minilm_l6_embed_func,  \"default\": default_embed_func }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3babfe5a",
   "metadata": {},
   "source": [
    "\n",
    "Durante el experimento, para cada modelo de embeddings se creará temporalmente una nueva tabla en LanceDB. En ella se añadirán los documentos. A continuación, realizaremos una consulta a partir de ella y examinaremos los resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9114cfbf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-20T07:13:56.829960Z",
     "start_time": "2023-07-20T07:13:56.825481Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "use_existing_table = False  # Esto sirve para indicar que crearemos una tabla al vuelo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0604f673",
   "metadata": {},
   "source": [
    "Documentos que se añadirán a la base de datos. LanceDB también acepta otros formatos de conjuntos de datos como **pydict, pyarrow, Pydantic, o incluso ficheros parquet** (que se emplean mucho en Big Data).\n",
    "[Más información](https://lancedb.github.io/lancedb/guides/tables/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f7fda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = pd.DataFrame(\n",
    "    {\n",
    "        \"text\": [\"This is a document\", \"This is another document\", \"This is the document.\"],\n",
    "        \"metadatas\": [{\"source\": \"my_source\"}, {\"source\": \"my_source\"}, {\"source\": \"my_source\"}],\n",
    "        \"ids\": [\"id1\", \"id2\", \"id3\"],\n",
    "    }\n",
    ")\n",
    "\n",
    "query_args = {\"text\": [\"This is a query document\", \"This is a another query document\"], \"metric\": [\"cosine\", \"l2\"]}\n",
    "\n",
    "\n",
    "# Configuramos el experimento\n",
    "experiment = LanceDBExperiment(\n",
    "    data=data,\n",
    "    embedding_fns=emb_fns,\n",
    "    query_args=query_args,\n",
    ")\n",
    "\n",
    "# [Opcional] Argumentos de consulta avanzados\n",
    "# Nuestras consultas de prueba, junto con argumentos de consulta opcionales. La consulta LanceDB acepta algunos argumentos para personalizar la búsqueda:\n",
    "# métricas: «l2», «coseno», o «punto» (coseno por defecto)\n",
    "# filtro: Cláusula SQL where para filtrar los resultados de la búsqueda vectorial antes de aplicar el límite. (Ninguna por defecto)\n",
    "# limit: número de resultados a devolver (3 por defecto)\n",
    "\"\"\"\n",
    "query_args_adv = {\n",
    "                \"text\": [\"This is a query document\", \"This is a another query document\"], \n",
    "                \"metric\": [\"cosine\", \"l2\", \"dot\"],\n",
    "                \"filter\": [\"text IS NOT NULL\" , \"text LIKE '%document.%'\"]\n",
    "                }\n",
    "experiment = LanceDBExperiment(\n",
    "    data=data,\n",
    "    embedding_fns=emb_fns,\n",
    "    query_args=query_args_adv,\n",
    " \n",
    ")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3fa5450",
   "metadata": {},
   "source": [
    "Ejecutamos el experimento con esta llamada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b33130",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-20T07:16:21.469371Z",
     "start_time": "2023-07-20T07:16:21.462342Z"
    }
   },
   "outputs": [],
   "source": [
    "experiment.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b013dca",
   "metadata": {},
   "source": [
    "Visualizamos los resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c7e682",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266c13eb",
   "metadata": {},
   "source": [
    "## Evaluar los resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bebb8023",
   "metadata": {},
   "source": [
    "Para evaluar los resultados, definiremos una función de evaluación. A veces, se conoce el orden del documento más relevante dada una consulta, y se puede calcular la correlación entre el ranking esperado y el ranking real."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ddbb951",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "\n",
    "# Aquí definimos cual sería el ranking ideal, para poder contrastarlo con el ejecutado en el experimento\n",
    "EXPECTED_RANKING = {\n",
    "    \"This is a query document\": [\"id1\", \"id3\", \"id2\"],\n",
    "    \"This is a another query document\": [\"id2\", \"id3\", \"id1\"],\n",
    "}\n",
    "\n",
    "\n",
    "def measure_correlation(row: \"pandas.core.series.Series\", ranking_column_name: str = \"top doc ids\") -> float:\n",
    "    r\"\"\"\n",
    "    A simple test that compares the expected ranking for a given query with the actual ranking produced\n",
    "    by the embedding function being tested.\n",
    "    \"\"\"\n",
    "    input_query = row[\"text\"]\n",
    "    correlation, _ = stats.spearmanr(row[ranking_column_name], EXPECTED_RANKING[input_query])\n",
    "    return correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "974d6065",
   "metadata": {},
   "source": [
    "Finalmente visualizamos los resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80dfeec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "experiment.evaluate(\"ranking_correlation\", measure_correlation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d09c18e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "experiment.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f0ddb5",
   "metadata": {},
   "source": [
    "# Próximos pasos para practicar:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f2b544",
   "metadata": {},
   "source": [
    "Intenta adaptar este código para que los embeddings los importe nativamente desde lancedb. \n",
    "Referencias:\n",
    "- [Embeddings en LanceDB](https://lancedb.github.io/lancedb/embeddings/default_embedding_functions/)\n",
    "- [Más ejemplos de LanceDB](https://github.com/lancedb/vectordb-recipes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10815d3e",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
